{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import jsonlines\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('../data/gender/transactions.csv')\n",
    "target_data = pd.read_csv('../data//gender/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.term_id = transactions.term_id.fillna(\"UNK\")\n",
    "transactions['trans'] = \"mcc\" + transactions['mcc_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   client_id tr_datetime  mcc_code  tr_type  amount_rur term_id small_group\n",
      "0   39026145  0 10:23:26      4814     1030    -2245.92     UNK     mcc4814\n",
      "1   39026145  1 10:19:29      6011     7010    56147.89     UNK     mcc6011\n",
      "2   39026145  1 10:20:56      4829     2330   -56147.89     UNK     mcc4829\n",
      "3   39026145  1 10:39:54      5499     1010    -1392.47     UNK     mcc5499\n",
      "4   39026145  2 15:33:42      5499     1010     -920.83     UNK     mcc5499\n"
     ]
    }
   ],
   "source": [
    "data = transactions.rename(columns={'customer_id': 'client_id', 'trans':'small_group', 'amount':'amount_rur'})\n",
    "target_data = target_data.rename(columns={'customer_id':'client_id', 'gender':'bins'})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change transaction to numbers\n",
    "keys = np.unique(data.small_group)\n",
    "new_values = np.arange(0,len(keys), dtype=int)\n",
    "dictionary = dict(zip(keys, new_values))\n",
    "new_column = [dictionary[key] for key in list(data.small_group)]\n",
    "data.small_group = new_column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for language model (LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../../gender\n",
    "!mkdir ../../gender/target_clf\n",
    "!mkdir ../../gender/substitute_clf\n",
    "!mkdir ../../gender/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_slice_subsample(sub_data, cnt_min, cnt_max, split_count):\n",
    "    sub_datas = []\n",
    "    cnt_min = cnt_min if len(sub_data) > cnt_max else int(cnt_min*len(sub_data)/cnt_max)\n",
    "    cnt_max = cnt_max if len(sub_data) > cnt_max else len(sub_data)-1\n",
    "    split_count = split_count if len(sub_data) > cnt_max else int(len(sub_data)/cnt_max*split_count)\n",
    "    for i in range(0, split_count):\n",
    "        if cnt_min < cnt_max: \n",
    "            T_i = np.random.randint(cnt_min, cnt_max)\n",
    "            s = np.random.randint(0, len(sub_data)-T_i-1)\n",
    "            S_i = sub_data[s:s+T_i-1]\n",
    "            sub_datas.append(S_i)\n",
    "            \n",
    "    return sub_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(name, data, target):\n",
    "    len_ = len(np.unique(target.client_id))\n",
    "    dict_data = {}\n",
    "    with jsonlines.open(name, \"w\") as writer:\n",
    "        S = 0\n",
    "        for index, client_id in enumerate(np.unique(target.client_id)):\n",
    "            sys.stdout.write(\"\\r %d out of %d\" % (index, len_))\n",
    "\n",
    "            sub_data = data[data['client_id']==client_id]\n",
    "            sub_data_target = target[target['client_id']==client_id]\n",
    "\n",
    "            sub_datas = split_slice_subsample(sub_data, 25, 150, 30)\n",
    "             \n",
    "            for loc_data in sub_datas:\n",
    "                loc_dict = {\"transactions\": list(loc_data.small_group),\n",
    "                            \"amounts\": list(loc_data.amount_rur),\n",
    "                            \"label\": int(sub_data_target.bins),\n",
    "                            \"client_id\": int(client_id)}\n",
    "                S = S+ len(loc_data.small_group)\n",
    "                writer.write(loc_dict) \n",
    "           \n",
    "    print('mean length:', S/(len(sub_datas)* len((np.unique(target.client_id)))))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_lm(data, target_data):\n",
    "    target_data_train, target_data_valid = train_test_split(target_data, test_size=0.2, random_state=10, shuffle=True)\n",
    "    print('Create train set...')\n",
    "    create_set('../../gender0/lm/train.jsonl', data, target_data_train)\n",
    "    print('Create valid set...')\n",
    "    create_set('../../gender0/lm/valid.jsonl', data, target_data_valid)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train set...\n",
      " 6719 out of 6720mean length: 76.26872023809524\n",
      "Create valid set...\n",
      " 1679 out of 1680mean length: 76.90662698412699\n"
     ]
    }
   ],
   "source": [
    "split_data_lm(data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘gender_substitute_clf’: File exists\n",
      "mkdir: cannot create directory ‘target_clf’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir gender_substitute_clf\n",
    "!mkdir target_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data_test_sub, target_data_targetclf = train_test_split(target_data, test_size=0.65, random_state=10, shuffle=True)\n",
    "target_data_subclf, target_data_test = train_test_split(target_data_test_sub, test_size=2./7, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target_data, dir_):\n",
    "    target_data_train, target_data_valid = train_test_split(target_data, test_size=0.2, random_state=10, shuffle=True)\n",
    "    print('Create train set...')\n",
    "    create_set(dir_+'/'+'train.jsonl', data, target_data_train)\n",
    "    print('Create valid set...')\n",
    "    create_set(str(dir_)+'/'+'valid.jsonl', data, target_data_valid)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 839 out of 840mean length: 76.39734126984126\n"
     ]
    }
   ],
   "source": [
    "#create test set for both target and substitute classifiers\n",
    "create_set('../../gender/test.jsonl', data, target_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train set...\n",
      " 1679 out of 1680mean length: 76.74390873015874\n",
      "Create valid set...\n",
      " 419 out of 420mean length: 77.74626984126984\n"
     ]
    }
   ],
   "source": [
    "#create valid and train data for substitute classifier\n",
    "split_data(data, target_data_subclf, '../../gender/substitute_clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train set...\n",
      " 4367 out of 4368mean length: 76.0153540903541\n",
      "Create valid set...\n",
      " 1091 out of 1092mean length: 77.4495115995116\n"
     ]
    }
   ],
   "source": [
    "#create valid and train data for target classifier\n",
    "split_data(data, target_data_targetclf, '../../gender/target_clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
